library(tidyr)
# Define the newspapers and their directories
outlets <- list(
bbcnewsnight = "bsky_data/bbcnewsnight/posts/",
telegraph = "bsky_data/tgraph/posts/",
guardian = "bsky_data/guardian/posts/"
)
# Initialize an empty list to store data
all_posts <- list()
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Load posts from each outlet
for (source in names(outlets)) {
post_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for users with no posts
users <- basename(post_files) %>% gsub("\\.rds$", "", .)
source_posts <- lapply(post_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data)) {
data <- data %>% mutate(source = source)  # Add source label
} else {
# Handle users with no posts
data <- data.frame(author_handle = gsub("\\.rds$", "", basename(file)), source = source)
}
return(data)
})
# Bind rows for each source
all_posts[[source]] <- bind_rows(source_posts)
}
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
# Ensure empty rows for users with no posts
all_users <- unique(combined_posts$author_handle)
combined_posts <- combined_posts %>%
group_by(author_handle) %>%
complete(author_handle = all_users, fill = list(source = NA)) %>%
ungroup()
# Ensure empty rows for users with no posts
all_users <- unique(combined_posts$author_handle)
combined_posts <- combined_posts %>%
group_by(author_handle) %>%
complete(author_handle = all_users, fill = list(source = NA)) %>%
ungroup()
# Remove duplicates
unique_posts <- combined_posts %>%
distinct(across(everything()))  # Adjust columns for uniqueness if needed
# Ensure empty rows for users with no posts
all_users <- unique(combined_posts$author_handle)
all_users
combined_posts <- combined_posts %>%
group_by(author_handle) %>%
complete(author_handle = all_users, fill = list(source = NA)) %>%
ungroup()
View(combined_posts)
empties <- combined_posts %>%
filter(is.na(text))
empties <- combined_posts %>%
filter(is.na(text))
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
empties <- combined_posts %>%
filter(is.na(text))
View(empties)
# Ensure all unique users are represented
all_users <- unique(combined_posts$author_handle)
# Create a data frame with all unique users and sources
empty_users <- data.frame(
author_handle = all_users,
source = NA,  # Placeholder for missing sources
stringsAsFactors = FALSE
)
# Bind rows for missing users and ensure all users are included
combined_posts <- bind_rows(combined_posts, empty_users) %>%
distinct(author_handle, .keep_all = TRUE)
View(combined_posts)
empties <- combined_posts %>%
filter(is.na(text))
View(combined_posts)
empties <- combined_posts %>%
filter(text=="")
# Bind rows for missing users and ensure all users are included
combined_posts <- bind_rows(empties) %>%
distinct(author_handle, .keep_all = TRUE)
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
empties <- combined_posts %>%
filter(text=="")
combined_posts %>%
filter(text=="")
# Load libraries
library(dplyr)
library(ggplot2)
library(cowplot)
library(lubridate)
library(tidyr)
# Define the newspapers and their directories
outlets <- list(
bbcnewsnight = "bsky_data/bbcnewsnight/posts/",
telegraph = "bsky_data/tgraph/posts/",
guardian = "bsky_data/guardian/posts/"
)
# Initialize an empty list to store data
all_posts <- list()
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Load posts from each outlet
for (source in names(outlets)) {
post_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for users with no posts
users <- basename(post_files) %>% gsub("\\.rds$", "", .)
source_posts <- lapply(post_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data)) {
data <- data %>% mutate(source = source)  # Add source label
} else {
# Handle users with no posts
data <- data.frame(author_handle = gsub("\\.rds$", "", basename(file)), source = source)
}
return(data)
})
# Bind rows for each source
all_posts[[source]] <- bind_rows(source_posts)
}
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
# Ensure empty rows for users with no posts
all_users <- unique(combined_posts$author_handle)
combined_posts <- combined_posts %>%
group_by(author_handle) %>%
complete(author_handle = all_users, fill = list(source = NA)) %>%
ungroup()
# Load libraries
library(dplyr)
library(ggplot2)
library(cowplot)
library(lubridate)
library(tidyr)
# Define the newspapers and their directories
outlets <- list(
bbcnewsnight = "bsky_data/bbcnewsnight/posts/",
telegraph = "bsky_data/tgraph/posts/",
guardian = "bsky_data/guardian/posts/"
)
# Initialize an empty list to store data
all_posts <- list()
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Load posts from each outlet
for (source in names(outlets)) {
post_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for users with no posts
users <- basename(post_files) %>% gsub("\\.rds$", "", .)
source_posts <- lapply(post_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data) && nrow(data) > 0) {
# Valid data with rows
data <- data %>% mutate(source = source)  # Add source label
} else {
# Handle empty or missing data
data <- data.frame(
author_handle = gsub("\\.rds$", "", basename(file)),  # Extract username from filename
source = source,
uri = NA,
cid = NA,
text = NA,
reply_count = NA,
repost_count = NA,
like_count = NA,
stringsAsFactors = FALSE
)
}
return(data)
})
# Bind rows for each source
all_posts[[source]] <- bind_rows(source_posts)
}
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
combined_posts %>%
filter(author_handle == "zzglenm.bsky.social")
# Save the combined dataset in the processed directory
saveRDS(combined_posts, file.path(processed_dir, "combined_posts.rds"))
# Load libraries
library(atrrr)
# Function to create directories
create_dir <- function(path) {
if (!dir.exists(path)) dir.create(path, recursive = TRUE)
}
# Define the outlets and their handles
outlets <- list(
bbcnewsnight = "bbcnewsnight.bsky.social",
guardian = 'did:plc:vovinwhtulbsx4mwfw26r5ni',
tgraph = "telegraphnews.bsky.social"
)
# Base directory for saving data
base_dir <- "bsky_data"
create_dir(base_dir)
# Get today's date for collection
collection_date <- Sys.Date()
# Function to safely save data as .rds
safe_save_rds <- function(data, path) {
tryCatch({
saveRDS(data, path)
}, error = function(e) {
message(sprintf("Error saving data to %s: %s", path, e$message))
})
}
# Loop through each outlet to get followers, posts, and user info
for (outlet in names(outlets)) {
outlet_handle <- outlets[[outlet]]
# Create directories with collection date suffix
outlet_dir <- file.path(base_dir, paste0(outlet, "_", collection_date))
create_dir(outlet_dir)
# Save followers
followers <- get_followers(actor = outlet_handle, limit = 10)$actor_handle
followers_path <- file.path(outlet_dir, "followers.rds")
safe_save_rds(data.frame(handle = followers), followers_path)
# Create subdirectories for posts and user info
posts_dir <- file.path(outlet_dir, "posts")
userinfo_dir <- file.path(outlet_dir, "userinfo")
create_dir(posts_dir)
create_dir(userinfo_dir)
# Get posts and user info for each follower
for (follower in followers) {
tryCatch({
# Get user info
user_info <- get_user_info(actor = follower)
userinfo_path <- file.path(userinfo_dir, paste0(follower, ".rds"))
safe_save_rds(user_info, userinfo_path)
# Get posts
user_posts <- get_skeets_authored_by(actor = follower, limit = 10L, parse = TRUE) #1500L limit for first run. 100kL thereafter
posts_path <- file.path(posts_dir, paste0(follower, ".rds"))
safe_save_rds(user_posts, posts_path)
}, error = function(e) {
message(sprintf("Error processing %s: %s", follower, e$message))
})
}
}
# Load libraries
library(dplyr)
library(ggplot2)
library(cowplot)
library(lubridate)
library(tidyr)
# Define the newspapers and their directories
outlets <- list(
bbcnewsnight = "bsky_data/bbcnewsnight_2024-11-18/posts/",
telegraph = "bsky_data/tgraph_2024-11-18/posts/",
guardian = "bsky_data/guardian_2024-11-18/posts/"
)
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Get today's date for processing
processing_date <- Sys.Date()
# Load posts from each outlet
for (source in names(outlets)) {
post_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for users with no posts
users <- basename(post_files) %>% gsub("\\.rds$", "", .)
source_posts <- lapply(post_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data) && nrow(data) > 0) {
# Valid data with rows
data <- data %>% mutate(source = source)  # Add source label
} else {
# Handle empty or missing data
data <- data.frame(
author_handle = gsub("\\.rds$", "", basename(file)),  # Extract username from filename
source = source,
uri = NA,
cid = NA,
text = NA,
reply_count = NA,
repost_count = NA,
like_count = NA,
stringsAsFactors = FALSE
)
}
return(data)
})
# Bind rows for each source
all_posts[[source]] <- bind_rows(source_posts)
}
# Combine all posts into one dataset
combined_posts <- bind_rows(all_posts)
# Create a filename with a date suffix
output_file <- file.path(processed_dir, paste0("combined_posts_", processing_date, ".rds"))
# Save the combined dataset with a date suffix
saveRDS(combined_posts, output_file)
# Initialize an empty list to store user information
all_userinfo <- list()
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Get today's date for processing
processing_date <- Sys.Date()
# Load user information from each outlet
for (source in names(outlets)) {
userinfo_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for missing user info
users <- basename(userinfo_files) %>% gsub("\\.rds$", "", .)
source_userinfo <- lapply(userinfo_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data) && nrow(data) > 0) {
# Valid data with rows
data <- data %>% mutate(source = source)  # Add source label
} else {
# Handle empty or missing data
data <- data.frame(
author_handle = gsub("\\.rds$", "", basename(file)),  # Extract username from filename
source = source,
user_name = NA,
bio = NA,
followers_count = NA,
following_count = NA,
stringsAsFactors = FALSE
)
}
return(data)
})
# Bind rows for each source
all_userinfo[[source]] <- bind_rows(source_userinfo)
}
# Combine all user information into one dataset
combined_userinfo <- bind_rows(all_userinfo)
View(combined_userinfo)
outlets <- list(
bbcnewsnight = "bsky_data/bbcnewsnight_2024-11-18/userinfo/",
telegraph = "bsky_data/tgraph_2024-11-18/userinfo/",
guardian = "bsky_data/guardian_2024-11-18/userinfo/"
)
# Initialize an empty list to store user information
all_userinfo <- list()
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Get today's date for processing
processing_date <- Sys.Date()
# Load user information from each outlet
for (source in names(outlets)) {
userinfo_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for missing user info
users <- basename(userinfo_files) %>% gsub("\\.rds$", "", .)
source_userinfo <- lapply(userinfo_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data) && nrow(data) > 0) {
# Valid data with rows
data <- data %>% mutate(source = source)  # Add source label
} else {
# Handle empty or missing data
data <- data.frame(
author_handle = gsub("\\.rds$", "", basename(file)),  # Extract username from filename
source = source,
user_name = NA,
bio = NA,
followers_count = NA,
following_count = NA,
stringsAsFactors = FALSE
)
}
return(data)
})
# Bind rows for each source
all_userinfo[[source]] <- bind_rows(source_userinfo)
}
# Combine all user information into one dataset
combined_userinfo <- bind_rows(all_userinfo)
View(combined_userinfo)
combined_userinfo$viewer_data
combined_userinfo$viewer_data[1]
# Create a filename with a date suffix
output_file <- file.path(processed_dir, paste0("combined_userinfo_", processing_date, ".rds"))
# Save the combined user information dataset with a date suffix
saveRDS(combined_userinfo, output_file)
test <- user_info <- get_user_info(actor = "cbarrie.bsky.social")
View(test)
outlets <- list(
bbcnewsnight = "bsky_data/bbcnewsnight_2024-11-18/userinfo/",
telegraph = "bsky_data/tgraph_2024-11-18/userinfo/",
guardian = "bsky_data/guardian_2024-11-18/userinfo/"
)
# Initialize an empty list to store user information
all_userinfo <- list()
# Create the processed directory
processed_dir <- "bsky_data/processed/"
if (!dir.exists(processed_dir)) dir.create(processed_dir, recursive = TRUE)
# Get today's date for processing
processing_date <- Sys.Date()
# Load user information from each outlet
for (source in names(outlets)) {
userinfo_files <- list.files(outlets[[source]], full.names = TRUE, pattern = "\\.rds$")
# Create a data frame for missing user info
users <- basename(userinfo_files) %>% gsub("\\.rds$", "", .)
source_userinfo <- lapply(userinfo_files, function(file) {
data <- readRDS(file)
if (is.data.frame(data) && nrow(data) > 0) {
# Valid data with rows
data <- data %>% mutate(source = source)  # Add source label
}
return(data)
})
# Bind rows for each source
all_userinfo[[source]] <- bind_rows(source_userinfo)
}
# Combine all user information into one dataset
combined_userinfo <- bind_rows(all_userinfo)
View(combined_userinfo)
# Create a filename with a date suffix
output_file <- file.path(processed_dir, paste0("combined_userinfo_", processing_date, ".rds"))
# Save the combined user information dataset with a date suffix
saveRDS(combined_userinfo, output_file)
test <- get_follows("cbarrie.bsky.social", limit = 2000L)
View(test)
test <- get_follows("cbarrie.bsky.social", limit = 2000L)$actor_handle
test
# Load libraries
library(dplyr)
library(lubridate)
# Define input file
userinfo_file <- "bsky_data/processed/combined_userinfo_2024-11-18.rds"
# Define base directory for saving follows data
base_dir <- "bsky_data/follows"
if (!dir.exists(base_dir)) dir.create(base_dir, recursive = TRUE)
# Get today's date for collection
collection_date <- Sys.Date()
# Function to safely save data as .rds
safe_save_rds <- function(data, path) {
tryCatch({
saveRDS(data, path)
}, error = function(e) {
message(sprintf("Error saving data to %s: %s", path, e$message))
})
}
# Load the user info dataset
userinfo <- readRDS(userinfo_file)
# Ensure the dataset contains the `author_handle` column
if (!"author_handle" %in% names(userinfo)) {
stop("The userinfo dataset must contain an 'author_handle' column.")
}
View(userinfo)
# Load libraries
library(dplyr)
library(lubridate)
# Define input file
userinfo_file <- "bsky_data/processed/combined_userinfo_2024-11-18.rds"
# Define base directory for saving follows data
base_dir <- "bsky_data/follows"
if (!dir.exists(base_dir)) dir.create(base_dir, recursive = TRUE)
# Get today's date for collection
collection_date <- Sys.Date()
# Function to safely save data as .rds
safe_save_rds <- function(data, path) {
tryCatch({
saveRDS(data, path)
}, error = function(e) {
message(sprintf("Error saving data to %s: %s", path, e$message))
})
}
# Load the user info dataset
userinfo <- readRDS(userinfo_file)
# Ensure the dataset contains the `actor_handle` column
if (!"author_handle" %in% names(userinfo)) {
stop("The userinfo dataset must contain an 'author_handle' column.")
}
View(userinfo)
# Load libraries
library(dplyr)
library(lubridate)
# Define input file
userinfo_file <- "bsky_data/processed/combined_userinfo_2024-11-18.rds"
# Define base directory for saving follows data
base_dir <- "bsky_data/follows"
if (!dir.exists(base_dir)) dir.create(base_dir, recursive = TRUE)
# Get today's date for collection
collection_date <- Sys.Date()
# Function to safely save data as .rds
safe_save_rds <- function(data, path) {
tryCatch({
saveRDS(data, path)
}, error = function(e) {
message(sprintf("Error saving data to %s: %s", path, e$message))
})
}
# Load the user info dataset
userinfo <- readRDS(userinfo_file)
# Ensure the dataset contains the `actor_handle` column
if (!"actor_handle" %in% names(userinfo)) {
stop("The userinfo dataset must contain an 'author_handle' column.")
}
# Process each user to get and save follows
for (user in userinfo$actor_handle) {
tryCatch({
# Get follows
follows <- get_follows(actor = user, limit = 10000L)
# Create a directory for the user with the date suffix
user_dir <- file.path(base_dir, paste0(user, "_", collection_date))
if (!dir.exists(user_dir)) dir.create(user_dir, recursive = TRUE)
# Save the follows data
follows_path <- file.path(user_dir, paste0("follows_", collection_date, ".rds"))
safe_save_rds(data.frame(follows), follows_path)
# Message for successful processing
message(sprintf("Saved follows for user %s to %s", user, follows_path))
}, error = function(e) {
message(sprintf("Error processing follows for user %s: %s", user, e$message))
})
}
